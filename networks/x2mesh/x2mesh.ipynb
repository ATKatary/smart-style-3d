{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X2Mesh\n",
    "\n",
    "A user interface for stylizing 3D objects given a text prompt or an image. The interface comes with several models which tests can be ran on, running the imports cell will reveal which models are available for testing\n",
    "<br />\n",
    "Based on the work of [text2mesh](https://threedle.github.io/text2mesh/), the code has been modified to create a testing, parameter tuning, and visualizing pipeline. \n",
    "<br />\n",
    "All relevant code for this notebook can be found on the [IMAD](https://github.com/ATKatary/IMAD) github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mSuccess[./implementation/inputs/vase/vase_1/vase.obj is goo!]: Number of verticies (21311) == the number of normals (21311)\u001b[0m\n",
      "Found 1 working models in ./implementation/inputs/vase/vase_1\n"
     ]
    }
   ],
   "source": [
    "import shlex\n",
    "import subprocess\n",
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import pyplot as plt\n",
    "from implementation.helpers.general_helpers import *\n",
    "\n",
    "make_prompt = lambda content, style: [f\"A {content} made of {style} \",\n",
    "                                      f\"A {style}-shaped {content}\",\n",
    "                                      f\"{content} that looks like its made of {style}\",\n",
    "                                      f\"{content} in the style of {style}\",\n",
    "                                      f\"{content} in the style of beautiful {style}\",\n",
    "                                      f\"An artistic {content} that mimics beautiful {style}\"]\n",
    "home_dir = \"./implementation\"\n",
    "text2mesh_path = \"./text2mesh\"\n",
    "models_dir = f\"{home_dir}/inputs/vase/vase_1\"\n",
    "# models_dir = \"/home/ubuntu/imad/segmentation/models/vases\"\n",
    "models = get_valid_models(models_dir, models={}, text2mesh_path=text2mesh_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image2Mesh Testing\n",
    "#### Prameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_n_aug = 5                                                                  # number of augmentations we add to the image\n",
    "n_iter = 1000                                                                  # number of iterations to run the network for\n",
    "img_lr = 0.0005                                                                # learning rate\n",
    "img_sigma = 10.0                                                               # frequency of texture spreading across mesh\n",
    "img_lr_decay = 0.9                                                             # decay learning rate\n",
    "obj, obj_path = select_model(models)\n",
    "output_dir = \"./implementation/outputs/img2mesh/lamp/wood/wood1\"\n",
    "img_path = download(home_dir, fn=\"style_image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ./implementation/img2mesh.sh {obj_path} {output_dir} '{img_path}' {n_iter + 1} {text2mesh_path} {home_dir} {img_sigma} {img_lr_decay} {img_lr} {img_n_aug}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_image_path = f\"{output_dir}/{obj}_iters_style_5/iter_0.jpg\"\n",
    "after_image_path = f\"{output_dir}/{obj}_iters_style_5/iter_{(n_iter // 100) * 100}.jpg\"\n",
    "\n",
    "display([before_image_path, after_image_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text2Mesh Testing\n",
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The available models are:\n",
      "['vase']\n"
     ]
    }
   ],
   "source": [
    "run_all = \"n\"; style = \"wood\"; content = \"vase\"\n",
    "if run_all is None: run_all = input(\"Run all? (y/n): \")\n",
    "if style is None: style = input(\"What is the style that I should test? \")\n",
    "if content is None: content = input(\"What is the content that I should test? \")\n",
    "prompts = make_prompt(content, style)\n",
    "\n",
    "n_iter = 5 \n",
    "text_n_aug = 1\n",
    "text_lr = 0.0005\n",
    "text_sigma = 10.0\n",
    "text_lr_decay = 0.9\n",
    "output_dir = \"./implementation/outputs/text2mesh/vase/wood\"\n",
    "vertices_to_not_change = \"./implementation/inputs/vase/vase_1/min_y_vertices.txt\"\n",
    "if run_all == \"n\": \n",
    "    obj, obj_path = select_model(models, selected_model=\"vase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mSuccess[./implementation/inputs/vase/vase_1/vase.obj is goo!]: Number of verticies (21311) == the number of normals (21311)\u001b[0m\n",
      "Saving to ./implementation/outputs/text2mesh/vase/wood/vase_final_style_24 ...\n",
      "Saving to ./implementation/outputs/text2mesh/vase/wood/vase_iters_style_24 ...\n",
      "ModuleList(\n",
      "  (0): FourierFeatureTransform()\n",
      "  (1): Linear(in_features=515, out_features=256, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (6): ReLU()\n",
      "  (7): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (8): ReLU()\n",
      "  (9): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (10): ReLU()\n",
      ")\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=256, out_features=3, bias=True)\n",
      ")\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "1 vertices will not be changed\n",
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]/home/ubuntu/anaconda3/envs/ss3d/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[images] >> torch.Size([5, 3, 224, 224])\n",
      "torch.Size([5, 3, 224, 224]) torch.Size([1, 512])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[images] >> torch.Size([5, 3, 224, 224])\n",
      "[loss] >> tensor([-0.9624], device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>)\n",
      "[norm_loss] >> tensor([-1.6953], device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>)\n",
      "[light_shape] >> torch.Size([1, 1, 720, 1280])\n",
      "[image] >> torch.Size([1, 720, 1280, 3])\n",
      "[images] >> torch.Size([1, 3, 720, 1280])\n",
      " 17%|███████▌                                     | 1/6 [00:02<00:14,  2.91s/it][light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[images] >> torch.Size([5, 3, 224, 224])\n",
      "torch.Size([5, 3, 224, 224]) torch.Size([1, 512])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[images] >> torch.Size([5, 3, 224, 224])\n",
      "[loss] >> tensor([-1.0596], device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>)\n",
      "[norm_loss] >> tensor([-1.7734], device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>)\n",
      " 33%|███████████████                              | 2/6 [00:04<00:08,  2.06s/it][light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[images] >> torch.Size([5, 3, 224, 224])\n",
      "torch.Size([5, 3, 224, 224]) torch.Size([1, 512])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[images] >> torch.Size([5, 3, 224, 224])\n",
      "[loss] >> tensor([-1.0410], device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>)\n",
      "[norm_loss] >> tensor([-1.5938], device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>)\n",
      " 50%|██████████████████████▌                      | 3/6 [00:05<00:05,  1.69s/it][light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[images] >> torch.Size([5, 3, 224, 224])\n",
      "torch.Size([5, 3, 224, 224]) torch.Size([1, 512])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[images] >> torch.Size([5, 3, 224, 224])\n",
      "[loss] >> tensor([-1.0566], device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>)\n",
      "[norm_loss] >> tensor([-1.6875], device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>)\n",
      " 67%|██████████████████████████████               | 4/6 [00:06<00:03,  1.52s/it][light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[images] >> torch.Size([5, 3, 224, 224])\n",
      "torch.Size([5, 3, 224, 224]) torch.Size([1, 512])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[images] >> torch.Size([5, 3, 224, 224])\n",
      "[loss] >> tensor([-0.9844], device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>)\n",
      "[norm_loss] >> tensor([-1.6641], device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>)\n",
      " 83%|█████████████████████████████████████▌       | 5/6 [00:08<00:01,  1.44s/it][light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[images] >> torch.Size([5, 3, 224, 224])\n",
      "torch.Size([5, 3, 224, 224]) torch.Size([1, 512])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[light_shape] >> torch.Size([1, 1, 224, 224])\n",
      "[image] >> torch.Size([1, 224, 224, 3])\n",
      "[images] >> torch.Size([5, 3, 224, 224])\n",
      "[loss] >> tensor([-0.9727], device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>)\n",
      "[norm_loss] >> tensor([-1.6279], device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>)\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:09<00:00,  1.58s/it]\n",
      "[light_shape] >> torch.Size([1, 1, 720, 1280])\n",
      "[image] >> torch.Size([1, 720, 1280, 3])\n",
      "[images] >> torch.Size([1, 3, 720, 1280])\n"
     ]
    }
   ],
   "source": [
    "!bash ./implementation/text2mesh.sh {obj_path} {output_dir} '{prompts[0]}' {n_iter + 1} {text2mesh_path} {home_dir} {text_sigma} {text_lr_decay} {text_lr} {text_n_aug} {vertices_to_not_change}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_all == \"y\":\n",
    "    for obj, obj_path in models.items():\n",
    "        for prompt in prompts:\n",
    "            subprocess.call(shlex.split(f\"bash ./implementation/img2mesh.sh {obj_path} {output_dir} '{prompt}' {n_iter + 1} {text2mesh_path} {home_dir} {text_sigma} {text_lr_decay} {text_lr} {text_n_aug}\"))\n",
    "else: \n",
    "    for prompt in prompts:\n",
    "        subprocess.call(shlex.split(f\"bash ./implementation/img2mesh.sh {obj_path} {output_dir} '{prompt}' {n_iter + 1} {text2mesh_path} {home_dir} {text_sigma} {text_lr_decay} {text_lr} {text_n_aug}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_image_path = f\"{output_dir}/{obj}_iters_style_0/iter_0.jpg\"\n",
    "after_image_path = f\"{output_dir}/{obj}_iters_style_0/iter_{(n_iter // 100) * 100}.jpg\"\n",
    "\n",
    "display([before_image_path, after_image_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes\n",
    "The original [text2mesh](https://github.com/threedle/text2mesh) repositotory was cloned in the creation of this notebook  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cadefa720d1a2267f4d12d08d812560a64cfe891877bc388bf0e8af3e4846067"
  },
  "kernelspec": {
   "display_name": "Python3 (ss3d)",
   "language": "python",
   "name": "ss3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
